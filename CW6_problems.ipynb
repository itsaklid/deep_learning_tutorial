{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this classwork we will \n",
    "a) show how to load image data with Pytorch. Then we will \n",
    "\n",
    "b) build and train a feed forward and a convolutional neural network to classify images as Cat Vs No-Cat, \n",
    "\n",
    "c) learn how to use a validation set to evaluate the performance during training \n",
    "\n",
    "d) use regularization techniques to suppress overfitting \n",
    "\n",
    "e) save a trained model to infer on a testin dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from an hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = h5py.File('train_catvnoncat.h5', 'r')\n",
    "test_file = h5py.File('test_catvnoncat.h5', 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect how imbalanced the dataset is in terms of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, c = np.unique(train_file['train_set_y'][:]==1, return_counts=True)\n",
    "print(u,c)\n",
    "u, c = np.unique(test_file['test_set_y'][:]==1, return_counts=True)\n",
    "print(u,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pytorch we typically need a Dataset that is fed into a Dataloader. Then we iterate over the Dataloader object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx].permute(2, 0, 1), self.y[idx]\n",
    "    def __len__(self):\n",
    "        length = self.x.shape[0]\n",
    "        return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset(train_file['train_set_x'][:], train_file['train_set_y'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_set,\n",
    "            batch_size=8,\n",
    "            shuffle=True,\n",
    "            drop_last = True,\n",
    "        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Iterating the dataloader')\n",
    "e = iter(train_loader)\n",
    "print('Gives us a list of length', len(e))\n",
    "print('Which is the number of batches of data')\n",
    "e.next()\n",
    "samples = e.next()\n",
    "print('By taking one batch, we get a list of:', len(samples))\n",
    "print('with two tensors: 0th', samples[0].shape, '1st', samples[1].shape)\n",
    "cats = samples[1][:] == 1\n",
    "#plt.imshow(samples[0][5])\n",
    "idx_cats = np.where(cats==True)\n",
    "idx_nocat = np.where(cats==False)\n",
    "for c in idx_cats[0]:\n",
    "    plt.imshow(samples[0][c].to(dtype=torch.uint8).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    print('With label:', samples[1][c])\n",
    "for d in idx_nocat[0]:\n",
    "    plt.imshow(samples[0][d].to(dtype=torch.uint8).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    print('With label:', samples[1][d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first build a simple Feed Forward Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleFeedForward, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)        \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.l2(x)\n",
    "        #x = F.relu(x)        \n",
    "        out = self.l3(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npixels = 64\n",
    "input_size = (npixels)**2 \n",
    "hidden = 256\n",
    "num_classes =2\n",
    "epochs = 20\n",
    "lr = 10e-3\n",
    "\n",
    "# If you want to use a GPU \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleFeedForward(input_size, hidden, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steps = len(train_loader)\n",
    "\n",
    "ff_loss_history = []\n",
    "ff_acc_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        #For accuracy\n",
    "        n_correct = 0\n",
    "        n_samples = 0 \n",
    "        \n",
    "        images = images.to(device) #(B, C, H, W)  \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #convert images grey for FF network\n",
    "        images = torch.mean(images, dim=1) # (B, H, W)\n",
    "        images = images.reshape(-1, input_size) #(B, H*W)\n",
    "        \n",
    "        # here you predict\n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, \n",
    "                         labels)\n",
    "        total_loss += loss\n",
    "        \n",
    "        _, predictions = torch.max(y_pred, 1)\n",
    "            \n",
    "        #For accuracy\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        # Zeros the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        #Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples    \n",
    "    ff_loss_history.append(total_loss/i)\n",
    "    ff_acc_history.append(acc)\n",
    "    print(f'epoch:{epoch:.0f}|loss:{ff_loss_history[-1]:.2f}|accuracy = {acc:.2f}')\n",
    "    \n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's build a Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 2)        \n",
    "        self.conv2 = nn.Conv2d(8, 16, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16*15*15, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x))) # (B, C , H, W)\n",
    "        #print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x))) # (B, C , H, W)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 16*15*15) # (B, C*H*W)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(x.shape)\n",
    "        x = self.fc3(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConv().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "epochs = 20\n",
    "\n",
    "conv_loss_history = []\n",
    "conv_acc_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        n_correct = 0\n",
    "        n_samples = 0 \n",
    "        \n",
    "        images = images.to(device) #(B, C, H, W)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        y_pred = # YOUR CODE HERE \n",
    "        loss = # YOUR CODE HERE \n",
    "        _, predictions = # YOUR CODE HERE \n",
    "            \n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        optimizer.# YOUR CODE HERE \n",
    "        loss.# YOUR CODE HERE \n",
    "        optimizer.# YOUR CODE HERE \n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples    \n",
    "    conv_loss_history.append(loss)\n",
    "    conv_acc_history.append(acc)\n",
    "    print(f'epoch:{epoch:.0f}|loss:{loss.item():.2f}|accuracy = {acc:.2f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the results for both  the Feed forward and the Convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs), ff_loss_history, label='Feed Forward')\n",
    "plt.plot(np.arange(epochs), conv_loss_history, label='Convolutional')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs), ff_acc_history, label='Feed Forward')\n",
    "plt.plot(np.arange(epochs), conv_acc_history, label='Convolutional')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same for the Fashion-MNIST dataset. The Cat no-Cat datasets is an easy one to classify but quite small for the results to be trusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load or download the FashionMNIST dataset\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
    "                                                transforms.Compose([transforms.ToTensor()]))\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
    "                                               transforms.Compose([transforms.ToTensor()])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = # YOUR CODE HERE (\n",
    "            train_set,\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            drop_last = True,\n",
    "        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inpect the dataloader again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iterating the dataloader')\n",
    "e = iter(train_loader)\n",
    "print('Gives us a list of length', len(e))\n",
    "print('Which is the number of batches of data')\n",
    "e.next()\n",
    "samples = e.next()\n",
    "print('By taking one batch, we get a list of:', len(samples))\n",
    "print('with two tensors: 0th', samples[0].shape, '1st', samples[1].shape)\n",
    "for i, c in enumerate(samples[0]):\n",
    "    plt.imshow(torch.squeeze(c))\n",
    "    plt.show()\n",
    "    print('With label:', samples[1][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npixels = 28\n",
    "input_size = (npixels)**2 \n",
    "hidden = 256\n",
    "num_classes =10\n",
    "epochs = 10\n",
    "lr = 10e-3\n",
    "\n",
    "# If you want to use a GPU \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleFeedForward(input_size, hidden, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steps = len(train_loader)\n",
    "\n",
    "ff_loss_history = []\n",
    "ff_acc_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        #For accuracy\n",
    "        n_correct = 0\n",
    "        n_samples = 0 \n",
    "        \n",
    "        images = images.to(device) #(B, C, H, W)  \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #convert images grey for FF network\n",
    "        images = torch.mean(images, dim=1) # (B, H, W)\n",
    "        images = images.reshape(-1, input_size) #(B, H*W)\n",
    "        \n",
    "        # here you predict\n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, \n",
    "                         labels)\n",
    "        total_loss += loss\n",
    "        \n",
    "        _, predictions = torch.max(y_pred, 1)\n",
    "            \n",
    "        #For accuracy\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        # Zeros the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        #Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples    \n",
    "    ff_loss_history.append(total_loss/i)\n",
    "    ff_acc_history.append(acc)\n",
    "    print(f'epoch:{epoch:.0f}|loss:{ff_loss_history[-1]:.2f}|accuracy = {acc:.2f}')\n",
    "    \n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 2)        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16*13*13, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x))) # (B, C, H, W)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 16*13*13) # (B, C*H*W)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = # YOUR CODE HERE\n",
    "criterion = nn.# YOUR CODE HERE\n",
    "optimizer = torch.# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "conv_loss_history = []\n",
    "conv_acc_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        n_correct = 0\n",
    "        n_samples = 0 \n",
    "        \n",
    "        images = images.to(device) #(B, C, H, W)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        total_loss += loss\n",
    "        \n",
    "        _, predictions = torch.max(y_pred, 1)\n",
    "            \n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples    \n",
    "    conv_loss_history.append(total_loss/i)\n",
    "    conv_acc_history.append(acc)\n",
    "    print(f'epoch:{epoch:.0f}|loss:{conv_loss_history[-1]:.2f}|accuracy = {acc:.2f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs), ff_loss_history, label='Feed Forward')\n",
    "plt.plot(np.arange(epochs), conv_loss_history, label='Convolutional')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs), ff_acc_history, label='Feed Forward')\n",
    "plt.plot(np.arange(epochs), conv_acc_history, label='Convolutional')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's see how a validation set can be used to monitor the overfitting of a model during training. We will stick to the convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    train_set,\n",
    "    [int(len(train_set)*0.8), int(len(train_set)*0.2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "            # YOUR CODE HERE,\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            drop_last = True,\n",
    "        ) \n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "            # YOUR CODE HERE,\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            drop_last = True,\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleConv().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        train_correct = 0\n",
    "        train_samples = 0 \n",
    "\n",
    "        \n",
    "        images = images.to(device) #(B, C, H, W)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        y_pred = model(images)\n",
    "        train_loss = criterion(y_pred, labels)\n",
    "        total_loss += train_loss\n",
    "        \n",
    "        _, predictions = torch.max(y_pred, 1)\n",
    "            \n",
    "        train_samples += labels.shape[0]\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_acc = 100.0 * train_correct / train_samples\n",
    "    train_loss_history.append(total_loss/i)\n",
    "    train_acc_history.append(train_acc)    \n",
    "\n",
    "    # Now do the validation\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    with torch.no_grad():\n",
    "        val_correct = 0\n",
    "        val_samples = 0 \n",
    "        for i, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device) #(B, C, H, W)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            y_pred = # YOUR CODE HERE \n",
    "            val_loss = # YOUR CODE HERE\n",
    "            total_loss += val_loss\n",
    "            _, predictions = torch.max(y_pred, 1)\n",
    "            \n",
    "            val_samples += labels.shape[0]\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "        val_acc = 100.0 * val_correct / val_samples\n",
    "        val_acc_history.append(val_acc)\n",
    "        val_loss_history.append(total_loss/i)\n",
    "        \n",
    "        print(f'Epoch= {epoch:.0f}')\n",
    "        print(f'|Training|Loss:{train_loss_history[-1]:.2f}|Acc:{train_acc:.2f}|')\n",
    "        print(f'|Validation|Loss:{val_loss_history[-1]:.2f}|Acc:{val_acc:.2f}|')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs), train_loss_history, label='Training')\n",
    "plt.plot(np.arange(epochs), val_loss_history, label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs), train_acc_history, label='Training')\n",
    "plt.plot(np.arange(epochs), val_acc_history, label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's modify the existing convolutional network adding some regularization. \n",
    "Dropout, batch norm, weight initialization, L2 regulatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvanceConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvanceConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 3, 2)\n",
    "        self.fc1 = nn.Linear(16*13*13, 128)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.bn =nn.BatchNorm1d(128)       \n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 16*13*13)\n",
    "        #print(x.shape)\n",
    "        x = F.relu((self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        #print(x.shape)\n",
    "        x = self.fc3(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AdvanceConv().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=10e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "    \n",
    "        train_correct = 0\n",
    "        train_samples = 0 \n",
    "    \n",
    "        images = images.to(device) #(B, C, H, W)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        y_pred = model(images)\n",
    "        train_loss = criterion(y_pred, labels)\n",
    "        total_loss += train_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predictions = torch.max(y_pred, 1)\n",
    "            \n",
    "        train_samples += labels.shape[0]\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    \n",
    "    train_acc = 100.0 * train_correct / train_samples\n",
    "    train_loss_history.append(total_loss/i)\n",
    "    train_acc_history.append(train_acc)    \n",
    "    \n",
    "    total_loss =0\n",
    "    with torch.no_grad():\n",
    "        val_correct = 0\n",
    "        val_samples = 0 \n",
    "        for i, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device) #(B, C, H, W)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            y_pred = model(images)\n",
    "            val_loss = criterion(y_pred, labels)\n",
    "            total_loss += val_loss\n",
    "            _, predictions = torch.max(y_pred, 1)\n",
    "            \n",
    "            val_samples += labels.shape[0]\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        val_acc = 100.0 * val_correct / val_samples\n",
    "        val_acc_history.append(val_acc)\n",
    "        val_loss_history.append(total_loss/i)\n",
    "        \n",
    "    print(f'Epoch= {epoch:.0f}')\n",
    "    print(f'|Training|Loss:{train_loss_history[-1]:.2f}|Acc:{train_acc:.2f}|')\n",
    "    print(f'|Validation|Loss:{val_loss_history[-1]:.2f}|Acc:{val_acc:.2f}|')\n",
    "                         \n",
    "torch.save(model.state_dict(), 'best_model.pt')           \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs), train_loss_history, label='Training')\n",
    "plt.plot(np.arange(epochs), val_loss_history, label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs), train_acc_history, label='Training')\n",
    "plt.plot(np.arange(epochs), val_acc_history, label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdvanceConv().to(device)\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set = dataset(test_file['test_set_x'][:], test_file['test_set_y'][:])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_set,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            drop_last = False,\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_correct = 0\n",
    "    test_samples = 0 \n",
    "    total_loss = 0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        \n",
    "        images = images.to(device) #(B, C, H, W)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        y_pred = model(images)\n",
    "        test_loss = criterion(y_pred, labels)\n",
    "        total_loss += test_loss\n",
    "        _, predictions = torch.max(y_pred, 1)\n",
    "\n",
    "        test_samples += labels.shape[0]\n",
    "        test_correct += (predictions == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100.0 * val_correct / val_samples\n",
    "    print(i)\n",
    "    total_loss = total_loss/i\n",
    "\n",
    "    print('Inference')\n",
    "    print(f'|Testing|Loss:{total_loss:.2f}|Acc:{test_acc:.2f}|')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
